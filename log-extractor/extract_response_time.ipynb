{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 FINAL CORRECTED RESPONSE TIME EXTRACTION\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "print(\"🔧 FINAL CORRECTED RESPONSE TIME EXTRACTION\")\n",
        "print(\"=\" * 55)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extraction function defined\n"
          ]
        }
      ],
      "source": [
        "def extract_log_data_final(log_line):\n",
        "    \"\"\"Final corrected extraction function for Apache logs with response times\"\"\"\n",
        "    \n",
        "    # Extract response time from the end (format: **seconds/microseconds**)\n",
        "    time_match = re.search(r'\\*\\*(\\d+)/(\\d+)\\*\\*$', log_line)\n",
        "    \n",
        "    if not time_match:\n",
        "        return None\n",
        "    \n",
        "    time_seconds = float(time_match.group(1))\n",
        "    time_microseconds = float(time_match.group(2))\n",
        "    \n",
        "    # Convert to total milliseconds: (seconds * 1000) + (microseconds / 1000)\n",
        "    total_response_time_ms = (time_seconds * 1000) + (time_microseconds / 1000)\n",
        "    \n",
        "    # Extract IP (first field)\n",
        "    ip_match = re.match(r'^(\\S+)', log_line)\n",
        "    ip = ip_match.group(1) if ip_match else ''\n",
        "    \n",
        "    # Extract timestamp\n",
        "    timestamp_match = re.search(r'\\[([^\\]]+)\\]', log_line)\n",
        "    timestamp = timestamp_match.group(1) if timestamp_match else ''\n",
        "    \n",
        "    # Extract HTTP method, URL, and protocol from the quoted request\n",
        "    request_match = re.search(r'\"(\\w+)\\s+([^\\s\"]+)\\s+([^\"]+)\"', log_line)\n",
        "    if request_match:\n",
        "        method = request_match.group(1)\n",
        "        url = request_match.group(2)\n",
        "        protocol = request_match.group(3)\n",
        "    else:\n",
        "        method = ''\n",
        "        url = ''\n",
        "        protocol = ''\n",
        "    \n",
        "    # Extract status code and response size\n",
        "    status_size_match = re.search(r'\" (\\d+) (\\d+) \"', log_line)\n",
        "    if status_size_match:\n",
        "        status = status_size_match.group(1)\n",
        "        response_size = status_size_match.group(2)\n",
        "    else:\n",
        "        status = ''\n",
        "        response_size = ''\n",
        "    \n",
        "    # Clean URL (remove query parameters)\n",
        "    if url and '?' in url:\n",
        "        url = url.split('?')[0]\n",
        "    \n",
        "    return {\n",
        "        'timestamp': timestamp,\n",
        "        'ip': ip,\n",
        "        'method': method,\n",
        "        'url': url,\n",
        "        'status': status,\n",
        "        'response_size': response_size,\n",
        "        'response_time_seconds': time_seconds,\n",
        "        'response_time_microseconds': time_microseconds,\n",
        "        'total_response_time_ms': total_response_time_ms\n",
        "    }\n",
        "\n",
        "print(\"✅ Extraction function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 TESTING FINAL EXTRACTION FUNCTION\n",
            "=============================================\n",
            "✅ Test successful!\n",
            "  IP: 10.103.14.12\n",
            "  Method: POST\n",
            "  URL: /trx_rajal/order_pmr/getDokter/\n",
            "  Status: 200\n",
            "  Response Size: 4245\n",
            "  Response Time: 16.667 ms\n"
          ]
        }
      ],
      "source": [
        "# Test the extraction function first\n",
        "print(\"🧪 TESTING FINAL EXTRACTION FUNCTION\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Test with actual log line format\n",
        "test_line = '10.103.14.12 - - [27/Aug/2024:06:26:14 +0700] \"POST /trx_rajal/order_pmr/getDokter/ HTTP/2.0\" 200 4245 \"https://simrs.rsmoewardi.com/home\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\" **0/16667**'\n",
        "\n",
        "result = extract_log_data_final(test_line)\n",
        "if result:\n",
        "    print(\"✅ Test successful!\")\n",
        "    print(f\"  IP: {result['ip']}\")\n",
        "    print(f\"  Method: {result['method']}\")\n",
        "    print(f\"  URL: {result['url']}\")\n",
        "    print(f\"  Status: {result['status']}\")\n",
        "    print(f\"  Response Size: {result['response_size']}\")\n",
        "    print(f\"  Response Time: {result['total_response_time_ms']:.3f} ms\")\n",
        "else:\n",
        "    print(\"❌ Test failed!\")\n",
        "    raise Exception(\"Extraction test failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processing function defined\n"
          ]
        }
      ],
      "source": [
        "def process_log_file_final(input_file, max_records=None):\n",
        "    \"\"\"Process log file with final corrected extraction\"\"\"\n",
        "    extracted_data = []\n",
        "    error_count = 0\n",
        "    \n",
        "    print(f\"  Processing: {input_file.name}\")\n",
        "    \n",
        "    with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            # Stop if we've reached the max records limit\n",
        "            if max_records and len(extracted_data) >= max_records:\n",
        "                print(f\"    -> Reached max records limit: {max_records:,}\")\n",
        "                break\n",
        "                \n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "                \n",
        "            try:\n",
        "                log_data = extract_log_data_final(line)\n",
        "                if log_data and log_data['ip'] and log_data['url']:\n",
        "                    extracted_data.append([\n",
        "                        log_data['timestamp'],\n",
        "                        log_data['ip'],\n",
        "                        log_data['method'],\n",
        "                        log_data['url'],\n",
        "                        log_data['status'],\n",
        "                        log_data['response_size'],\n",
        "                        log_data['response_time_seconds'],\n",
        "                        log_data['response_time_microseconds'],\n",
        "                        log_data['total_response_time_ms']\n",
        "                    ])\n",
        "                else:\n",
        "                    error_count += 1\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 3:  # Only print first few errors\n",
        "                    print(f\"    Error on line {line_num}: {e}\")\n",
        "                continue\n",
        "            \n",
        "            # Progress update every 100k records\n",
        "            if len(extracted_data) % 100000 == 0:\n",
        "                print(f\"    -> Progress: {len(extracted_data):,} records extracted\")\n",
        "    \n",
        "    print(f\"    -> Final: {len(extracted_data):,} records, Errors: {error_count:,}\")\n",
        "    return extracted_data\n",
        "\n",
        "print(\"✅ Processing function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Found 15 log files:\n",
            "   1. response_time-simrs.log.10 (1014.7 MB)\n",
            "   2. response_time-simrs.log.11 (1070.6 MB)\n",
            "   3. response_time-simrs.log.9 (1048.5 MB)\n",
            "   4. response_time-simrs.log.7 (921.3 MB)\n",
            "   5. response_time-simrs.log.1 (1049.0 MB)\n",
            "   6. response_time-simrs.log.6 (511.5 MB)\n",
            "   7. response_time-simrs.log.8 (1045.0 MB)\n",
            "   8. response_time-simrs.log.13 (549.2 MB)\n",
            "   9. response_time-simrs.log.14 (990.9 MB)\n",
            "  10. response_time-simrs.log.12 (566.3 MB)\n",
            "  11. response_time-simrs.log (623.3 MB)\n",
            "  12. response_time-simrs.log.3 (1167.4 MB)\n",
            "  13. response_time-simrs.log.4 (1044.2 MB)\n",
            "  14. response_time-simrs.log.5 (526.1 MB)\n",
            "  15. response_time-simrs.log.2 (1109.6 MB)\n",
            "\n",
            "📊 Total size: 12.93 GB\n"
          ]
        }
      ],
      "source": [
        "# Check available log files\n",
        "logs_folder = Path('../Data')\n",
        "log_files = list(logs_folder.glob('response_time-simrs*'))\n",
        "\n",
        "print(f\"📂 Found {len(log_files)} log files:\")\n",
        "for i, log_file in enumerate(log_files, 1):\n",
        "    file_size_mb = log_file.stat().st_size / (1024 * 1024)\n",
        "    print(f\"  {i:2d}. {log_file.name} ({file_size_mb:.1f} MB)\")\n",
        "\n",
        "print(f\"\\n📊 Total size: {sum(f.stat().st_size for f in log_files) / (1024 * 1024 * 1024):.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 OPTION 1: Process single file (recommended for testing)\n",
            "============================================================\n",
            "Processing smallest file: response_time-simrs.log.6\n",
            "  Processing: response_time-simrs.log.6\n",
            "    -> Reached max records limit: 10,000\n",
            "    -> Final: 10,000 records, Errors: 0\n",
            "\n",
            "✅ Sample extraction successful!\n",
            "📊 Extracted 10,000 records\n",
            "💾 Sample saved to: sample_data_with_time.csv\n",
            "\n",
            "📈 Quick Analysis:\n",
            "  • Average response time: 813.08 ms\n",
            "  • Max response time: 50287.38 ms\n",
            "  • Min response time: 0.16 ms\n",
            "  • Records with > 0ms: 10,000\n"
          ]
        }
      ],
      "source": [
        "# OPTION 1: Process just one file first (for testing)\n",
        "print(\"🧪 OPTION 1: Process single file (recommended for testing)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Choose the smallest file for testing\n",
        "smallest_file = min(log_files, key=lambda f: f.stat().st_size)\n",
        "print(f\"Processing smallest file: {smallest_file.name}\")\n",
        "\n",
        "# Process with a limit to test\n",
        "sample_data = process_log_file_final(smallest_file, max_records=10000)\n",
        "\n",
        "if sample_data:\n",
        "    print(f\"\\n✅ Sample extraction successful!\")\n",
        "    print(f\"📊 Extracted {len(sample_data):,} records\")\n",
        "    \n",
        "    # Save sample to CSV\n",
        "    sample_output = 'sample_data_with_time.csv'\n",
        "    with open(sample_output, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            'Time', 'IP', 'Method', 'URL', 'Status', 'Response_Size',\n",
        "            'Response_Time_Seconds', 'Response_Time_Microseconds', 'Total_Response_Time_MS'\n",
        "        ])\n",
        "        writer.writerows(sample_data)\n",
        "    \n",
        "    print(f\"💾 Sample saved to: {sample_output}\")\n",
        "    \n",
        "    # Quick analysis\n",
        "    df_sample = pd.DataFrame(sample_data, columns=[\n",
        "        'Time', 'IP', 'Method', 'URL', 'Status', 'Response_Size',\n",
        "        'Response_Time_Seconds', 'Response_Time_Microseconds', 'Total_Response_Time_MS'\n",
        "    ])\n",
        "    \n",
        "    print(f\"\\n📈 Quick Analysis:\")\n",
        "    print(f\"  • Average response time: {df_sample['Total_Response_Time_MS'].mean():.2f} ms\")\n",
        "    print(f\"  • Max response time: {df_sample['Total_Response_Time_MS'].max():.2f} ms\")\n",
        "    print(f\"  • Min response time: {df_sample['Total_Response_Time_MS'].min():.2f} ms\")\n",
        "    print(f\"  • Records with > 0ms: {(df_sample['Total_Response_Time_MS'] > 0).sum():,}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Sample extraction failed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 OPTION 2: Process ALL files - EXTRACTION VERIFIED!\n",
            "======================================================================\n",
            "✅ Option 1 test successful! Response times working perfectly.\n",
            "📊 Expected: ~51 million records with response times\n",
            "⏱️  Estimated time: 30-60 minutes\n",
            "💾 Required: ~8GB RAM, ~5GB disk space\n",
            "🎯 Output: extracted_data_with_time_final.csv\n",
            "\n",
            "Processing file 1/15: response_time-simrs.log.10\n",
            "  Processing: response_time-simrs.log.10\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Final: 3,965,297 records, Errors: 0\n",
            "  ✅ Total extracted so far: 3,965,297\n",
            "\n",
            "Processing file 2/15: response_time-simrs.log.11\n",
            "  Processing: response_time-simrs.log.11\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Progress: 4,100,000 records extracted\n",
            "    -> Final: 4,172,337 records, Errors: 0\n",
            "  ✅ Total extracted so far: 8,137,634\n",
            "\n",
            "Processing file 3/15: response_time-simrs.log.9\n",
            "  Processing: response_time-simrs.log.9\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Final: 4,082,604 records, Errors: 0\n",
            "  ✅ Total extracted so far: 12,220,238\n",
            "\n",
            "Processing file 4/15: response_time-simrs.log.7\n",
            "  Processing: response_time-simrs.log.7\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Final: 3,553,690 records, Errors: 0\n",
            "  ✅ Total extracted so far: 15,773,928\n",
            "\n",
            "Processing file 5/15: response_time-simrs.log.1\n",
            "  Processing: response_time-simrs.log.1\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Final: 4,073,578 records, Errors: 0\n",
            "  ✅ Total extracted so far: 19,847,506\n",
            "\n",
            "Processing file 6/15: response_time-simrs.log.6\n",
            "  Processing: response_time-simrs.log.6\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Final: 2,002,360 records, Errors: 0\n",
            "  ✅ Total extracted so far: 21,849,866\n",
            "\n",
            "Processing file 7/15: response_time-simrs.log.8\n",
            "  Processing: response_time-simrs.log.8\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Final: 4,089,134 records, Errors: 0\n",
            "  ✅ Total extracted so far: 25,939,000\n",
            "\n",
            "Processing file 8/15: response_time-simrs.log.13\n",
            "  Processing: response_time-simrs.log.13\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Final: 2,147,149 records, Errors: 0\n",
            "  ✅ Total extracted so far: 28,086,149\n",
            "\n",
            "Processing file 9/15: response_time-simrs.log.14\n",
            "  Processing: response_time-simrs.log.14\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Final: 3,820,944 records, Errors: 0\n",
            "  ✅ Total extracted so far: 31,907,093\n",
            "\n",
            "Processing file 10/15: response_time-simrs.log.12\n",
            "  Processing: response_time-simrs.log.12\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Final: 2,213,930 records, Errors: 0\n",
            "  ✅ Total extracted so far: 34,121,023\n",
            "\n",
            "Processing file 11/15: response_time-simrs.log\n",
            "  Processing: response_time-simrs.log\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Final: 2,429,619 records, Errors: 0\n",
            "  ✅ Total extracted so far: 36,550,642\n",
            "\n",
            "Processing file 12/15: response_time-simrs.log.3\n",
            "  Processing: response_time-simrs.log.3\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Progress: 4,100,000 records extracted\n",
            "    -> Progress: 4,200,000 records extracted\n",
            "    -> Progress: 4,300,000 records extracted\n",
            "    -> Progress: 4,400,000 records extracted\n",
            "    -> Progress: 4,500,000 records extracted\n",
            "    -> Final: 4,549,183 records, Errors: 0\n",
            "  ✅ Total extracted so far: 41,099,825\n",
            "\n",
            "Processing file 13/15: response_time-simrs.log.4\n",
            "  Processing: response_time-simrs.log.4\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Final: 4,077,959 records, Errors: 0\n",
            "  ✅ Total extracted so far: 45,177,784\n",
            "\n",
            "Processing file 14/15: response_time-simrs.log.5\n",
            "  Processing: response_time-simrs.log.5\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Final: 2,064,488 records, Errors: 0\n",
            "  ✅ Total extracted so far: 47,242,272\n",
            "\n",
            "Processing file 15/15: response_time-simrs.log.2\n",
            "  Processing: response_time-simrs.log.2\n",
            "    -> Progress: 100,000 records extracted\n",
            "    -> Progress: 200,000 records extracted\n",
            "    -> Progress: 300,000 records extracted\n",
            "    -> Progress: 400,000 records extracted\n",
            "    -> Progress: 500,000 records extracted\n",
            "    -> Progress: 600,000 records extracted\n",
            "    -> Progress: 700,000 records extracted\n",
            "    -> Progress: 800,000 records extracted\n",
            "    -> Progress: 900,000 records extracted\n",
            "    -> Progress: 1,000,000 records extracted\n",
            "    -> Progress: 1,100,000 records extracted\n",
            "    -> Progress: 1,200,000 records extracted\n",
            "    -> Progress: 1,300,000 records extracted\n",
            "    -> Progress: 1,400,000 records extracted\n",
            "    -> Progress: 1,500,000 records extracted\n",
            "    -> Progress: 1,600,000 records extracted\n",
            "    -> Progress: 1,700,000 records extracted\n",
            "    -> Progress: 1,800,000 records extracted\n",
            "    -> Progress: 1,900,000 records extracted\n",
            "    -> Progress: 2,000,000 records extracted\n",
            "    -> Progress: 2,100,000 records extracted\n",
            "    -> Progress: 2,200,000 records extracted\n",
            "    -> Progress: 2,300,000 records extracted\n",
            "    -> Progress: 2,400,000 records extracted\n",
            "    -> Progress: 2,500,000 records extracted\n",
            "    -> Progress: 2,600,000 records extracted\n",
            "    -> Progress: 2,700,000 records extracted\n",
            "    -> Progress: 2,800,000 records extracted\n",
            "    -> Progress: 2,900,000 records extracted\n",
            "    -> Progress: 3,000,000 records extracted\n",
            "    -> Progress: 3,100,000 records extracted\n",
            "    -> Progress: 3,200,000 records extracted\n",
            "    -> Progress: 3,300,000 records extracted\n",
            "    -> Progress: 3,400,000 records extracted\n",
            "    -> Progress: 3,500,000 records extracted\n",
            "    -> Progress: 3,600,000 records extracted\n",
            "    -> Progress: 3,700,000 records extracted\n",
            "    -> Progress: 3,800,000 records extracted\n",
            "    -> Progress: 3,900,000 records extracted\n",
            "    -> Progress: 4,000,000 records extracted\n",
            "    -> Progress: 4,100,000 records extracted\n",
            "    -> Progress: 4,200,000 records extracted\n",
            "    -> Progress: 4,300,000 records extracted\n",
            "    -> Final: 4,303,419 records, Errors: 0\n",
            "  ✅ Total extracted so far: 51,545,691\n",
            "\n",
            "💾 Writing 51,545,691 records to extracted_data_with_time_final.csv\n",
            "✅ All files processed! Output: extracted_data_with_time_final.csv\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# OPTION 2: Process all files - READY TO RUN! \n",
        "print(\"🚀 OPTION 2: Process ALL files - EXTRACTION VERIFIED!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✅ Option 1 test successful! Response times working perfectly.\")\n",
        "print(\"📊 Expected: ~51 million records with response times\")\n",
        "print(\"⏱️  Estimated time: 30-60 minutes\")\n",
        "print(\"💾 Required: ~8GB RAM, ~5GB disk space\")\n",
        "print(\"🎯 Output: extracted_data_with_time_final.csv\")\n",
        "\n",
        "# READY TO PROCESS - Option 1 test was successful!\n",
        "process_all = True  # ENABLED - test was successful!\n",
        "\n",
        "if process_all:\n",
        "    all_extracted_data = []\n",
        "    \n",
        "    for i, log_file in enumerate(log_files, 1):\n",
        "        print(f\"\\nProcessing file {i}/{len(log_files)}: {log_file.name}\")\n",
        "        try:\n",
        "            extracted_data = process_log_file_final(log_file)\n",
        "            all_extracted_data.extend(extracted_data)\n",
        "            print(f\"  ✅ Total extracted so far: {len(all_extracted_data):,}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error processing {log_file}: {e}\")\n",
        "    \n",
        "    # Save to CSV\n",
        "    output_file = 'extracted_data_with_time_final.csv'\n",
        "    print(f\"\\n💾 Writing {len(all_extracted_data):,} records to {output_file}\")\n",
        "    \n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            'Time', 'IP', 'Method', 'URL', 'Status', 'Response_Size',\n",
        "            'Response_Time_Seconds', 'Response_Time_Microseconds', 'Total_Response_Time_MS'\n",
        "        ])\n",
        "        writer.writerows(all_extracted_data)\n",
        "    \n",
        "    print(f\"✅ All files processed! Output: {output_file}\")\n",
        "else:\n",
        "    print(\"🔄 Set process_all = True to run full extraction\")\n",
        "    print(\"📝 Recommended: Test with Option 1 first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 OPTION 3: Process files in BATCHES (recommended)\n",
            "=======================================================\n",
            "This approach processes files one by one and saves each separately\n",
            "This prevents memory issues and allows you to stop/resume\n",
            "🔄 Set batch_process = True to run batch extraction\n",
            "📝 This is the safest option for large datasets\n"
          ]
        }
      ],
      "source": [
        "# OPTION 3: Process files in batches (SAFER alternative)\n",
        "print(\"📦 OPTION 3: Process files in BATCHES (SAFER OPTION)\")\n",
        "print(\"=\" * 55)\n",
        "print(\"✅ This approach is SAFER for large datasets\")\n",
        "print(\"📁 Processes files one by one and saves each separately\")\n",
        "print(\"🔄 Prevents memory issues and allows you to stop/resume\")\n",
        "print(\"💡 Recommended if you have limited RAM (<8GB)\")\n",
        "\n",
        "batch_process = False  # Change to True if you prefer this safer approach\n",
        "batch_size_limit = None  # Process full files (remove limit)\n",
        "\n",
        "if batch_process:\n",
        "    for i, log_file in enumerate(log_files, 1):\n",
        "        output_file = f'extracted_data_batch_{i:02d}_{log_file.stem}.csv'\n",
        "        \n",
        "        print(f\"\\n📂 Processing batch {i}/{len(log_files)}: {log_file.name}\")\n",
        "        print(f\"💾 Output will be: {output_file}\")\n",
        "        \n",
        "        try:\n",
        "            # Process this file\n",
        "            extracted_data = process_log_file_final(log_file, max_records=batch_size_limit)\n",
        "            \n",
        "            if extracted_data:\n",
        "                # Save immediately\n",
        "                with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([\n",
        "                        'Time', 'IP', 'Method', 'URL', 'Status', 'Response_Size',\n",
        "                        'Response_Time_Seconds', 'Response_Time_Microseconds', 'Total_Response_Time_MS'\n",
        "                    ])\n",
        "                    writer.writerows(extracted_data)\n",
        "                \n",
        "                print(f\"  ✅ Saved {len(extracted_data):,} records to {output_file}\")\n",
        "                \n",
        "                # Quick stats\n",
        "                total_response_times = [row[8] for row in extracted_data]  # Total_Response_Time_MS\n",
        "                avg_time = sum(total_response_times) / len(total_response_times)\n",
        "                max_time = max(total_response_times)\n",
        "                print(f\"  📊 Avg: {avg_time:.2f}ms, Max: {max_time:.2f}ms\")\n",
        "            else:\n",
        "                print(f\"  ❌ No data extracted from {log_file.name}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error processing {log_file}: {e}\")\n",
        "            \n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    print(\"✅ Batch processing completed!\")\n",
        "    print(\"📁 Each file has been processed into a separate CSV\")\n",
        "    print(\"💡 You can now analyze individual batches or combine them later\")\n",
        "    \n",
        "else:\n",
        "    print(\"🔄 Set batch_process = True to run batch extraction\")\n",
        "    print(\"📝 This is the safest option for large datasets\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
