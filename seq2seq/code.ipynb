{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the IP addresses and URLs\n",
    "ip_tokenizer = Tokenizer()\n",
    "url_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizers on the IP and URL columns\n",
    "ip_tokenizer.fit_on_texts(data['IP'])\n",
    "url_tokenizer.fit_on_texts(data['URL'])\n",
    "\n",
    "# Convert IP and URL into sequences of tokens\n",
    "data['ip_seq'] = ip_tokenizer.texts_to_sequences(data['IP'])\n",
    "data['url_seq'] = url_tokenizer.texts_to_sequences(data['URL'])\n",
    "\n",
    "# Create sequences for training\n",
    "sequence_length = 3\n",
    "\n",
    "def create_sequences(data):\n",
    "    sequences = []\n",
    "    next_url = []\n",
    "    ips = []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data['url_seq'][i:i + sequence_length])\n",
    "        next_url.append(data['url_seq'][i + sequence_length])\n",
    "        ips.append(data['ip_seq'][i])\n",
    "    \n",
    "    return sequences, next_url, ips\n",
    "\n",
    "sequences, next_url, ips = create_sequences(data)\n",
    "\n",
    "# Pad sequences to make sure they are of uniform length\n",
    "X_url = pad_sequences(sequences, maxlen=sequence_length, padding='post')\n",
    "X_ip = pad_sequences(ips, maxlen=1)  # Assuming each IP is treated as a single token\n",
    "y = pad_sequences(next_url, maxlen=1, padding='post')\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "X_train_url, X_test_url, X_train_ip, X_test_ip, y_train, y_test = train_test_split(X_url, X_ip, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Dense, Embedding, Concatenate\n",
    "\n",
    "# Define the input layers\n",
    "url_input = Input(shape=(sequence_length,), name='url_input')\n",
    "ip_input = Input(shape=(1,), name='ip_input')\n",
    "\n",
    "# Embedding layers for URLs and IPs\n",
    "num_url_tokens = len(url_tokenizer.word_index) + 1\n",
    "num_ip_tokens = len(ip_tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "\n",
    "url_embedding = Embedding(input_dim=num_url_tokens, output_dim=embedding_dim)(url_input)\n",
    "ip_embedding = Embedding(input_dim=num_ip_tokens, output_dim=embedding_dim)(ip_input)\n",
    "\n",
    "# Concatenate URL and IP embeddings\n",
    "concat = Concatenate()([url_embedding, ip_embedding])\n",
    "\n",
    "# GRU Layer\n",
    "gru_output = GRU(256)(concat)\n",
    "\n",
    "# Output layer (predict next URL)\n",
    "output = Dense(num_url_tokens, activation='softmax')(gru_output)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[url_input, ip_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit([X_train_url, X_train_ip], y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate([X_test_url, X_test_ip], y_test)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model to a file after training\n",
    "model.save('url_prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the model from file\n",
    "loaded_model = load_model('url_prediction_model.h5')\n",
    "\n",
    "# Function to make a prediction\n",
    "def predict_next_url(loaded_model, ip, url_sequence):\n",
    "    ip_seq = ip_tokenizer.texts_to_sequences([ip])\n",
    "    url_seq = url_tokenizer.texts_to_sequences(url_sequence)\n",
    "    \n",
    "    # Pad sequences\n",
    "    url_seq = pad_sequences(url_seq, maxlen=sequence_length)\n",
    "    ip_seq = pad_sequences(ip_seq, maxlen=1)\n",
    "    \n",
    "    # Predict next URL\n",
    "    pred = loaded_model.predict([url_seq, ip_seq])\n",
    "    \n",
    "    # Convert prediction back to URL\n",
    "    pred_url_index = np.argmax(pred, axis=1)[0]\n",
    "    pred_url = url_tokenizer.index_word.get(pred_url_index, \"Unknown URL\")\n",
    "    return pred_url\n",
    "\n",
    "# Test the prediction using a sample input\n",
    "sample_ip = \"10.119.17.11\"\n",
    "sample_urls = [\"/trx_rajal/trx_rajal/rptHasilLabFrame/2408261537/LK2408260198\", \n",
    "               \"/trx_rajal/trx_rajal/rptHasilLab/2408261537/LK2408260198\"]\n",
    "\n",
    "predicted_url = predict_next_url(loaded_model, sample_ip, sample_urls)\n",
    "print(f\"Predicted next URL: {predicted_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
