{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset setelah menghapus kelas dengan jumlah data < 2: 870656 baris\n",
      "Jumlah data training: 696524\n",
      "Jumlah data testing: 174132\n",
      "Data training dan testing disimpan ke file.\n"
     ]
    }
   ],
   "source": [
    "# Baca file dataset\n",
    "file_path = \"../log-extractor/extracted_data_normalized_filtered_part1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Pastikan kolom yang diperlukan tersedia\n",
    "if not {'IP', 'Normalized_URL', 'Time'}.issubset(df.columns):\n",
    "    raise ValueError(\"Kolom yang diperlukan tidak ditemukan dalam dataset.\")\n",
    "\n",
    "# Konversi waktu dan urutkan berdasarkan waktu\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%d/%b/%Y:%H:%M:%S %z')\n",
    "df = df.sort_values(by='Time')\n",
    "\n",
    "# Hitung jumlah URL unik\n",
    "url_counts = df['Normalized_URL'].value_counts()\n",
    "\n",
    "# Hanya pertahankan URL dengan jumlah data >= 2\n",
    "valid_urls = url_counts[url_counts >= 2].index\n",
    "df = df[df['Normalized_URL'].isin(valid_urls)]\n",
    "\n",
    "print(f\"Dataset setelah menghapus kelas dengan jumlah data < 2: {len(df)} baris\")\n",
    "\n",
    "# Stratified sampling berdasarkan kolom 'Normalized_URL'\n",
    "training_data, testing_data = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,  # 20% untuk testing\n",
    "    stratify=df['Normalized_URL'],  # Stratify berdasarkan URL\n",
    "    random_state=42  # Seed untuk hasil sampling konsisten\n",
    ")\n",
    "\n",
    "print(f\"Jumlah data training: {len(training_data)}\")\n",
    "print(f\"Jumlah data testing: {len(testing_data)}\")\n",
    "\n",
    "# Simpan hasil pembagian ke file (opsional)\n",
    "training_data.to_csv(\"../log-extractor/training_data.csv\", index=False)\n",
    "testing_data.to_csv(\"../log-extractor/testing_data.csv\", index=False)\n",
    "print(\"Data training dan testing disimpan ke file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membangun Markov Chain\n",
    "def build_markov_chain(training_data, order=1):\n",
    "    transitions = defaultdict(lambda: defaultdict(int))\n",
    "    grouped = training_data.groupby('IP')\n",
    "\n",
    "    for ip, group in grouped:\n",
    "        urls = group['Normalized_URL'].tolist()\n",
    "        for i in range(len(urls) - order):\n",
    "            state = tuple(urls[i:i+order])  # State terdiri dari 'order' URL terakhir\n",
    "            next_url = urls[i+order]\n",
    "            transitions[state][next_url] += 1\n",
    "\n",
    "    # Konversi ke probabilitas\n",
    "    markov_model = {}\n",
    "    for state, next_urls in transitions.items():\n",
    "        total_transitions = sum(next_urls.values())\n",
    "        markov_model[state] = {url: count / total_transitions for url, count in next_urls.items()}\n",
    "    \n",
    "    return markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dibangun Markov chains untuk orde 1 hingga 4\n"
     ]
    }
   ],
   "source": [
    "# Bangun Markov Chains Multi-Orde\n",
    "markov_models = {}\n",
    "max_order = 4  # Anda dapat menyesuaikan max_order\n",
    "for order in range(1, max_order + 1):\n",
    "    markov_models[order] = build_markov_chain(training_data, order=order)\n",
    "\n",
    "print(f\"Dibangun Markov chains untuk orde 1 hingga {max_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nn_dataset(testing_data, markov_models, max_order=4):\n",
    "    \"\"\"\n",
    "    Siapkan dataset untuk Neural Network dari data testing dan Markov Chains.\n",
    "    \n",
    "    Args:\n",
    "        testing_data (DataFrame): Data testing dengan kolom 'IP' dan 'Normalized_URL'.\n",
    "        markov_models (dict): Multi-Orde Markov Chains yang sudah dibangun.\n",
    "        max_order (int): Orde maksimum dari Markov Chains yang digunakan.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y), di mana:\n",
    "               - X adalah array input untuk Neural Network.\n",
    "               - y adalah array target (URL berikutnya).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    grouped = testing_data.groupby('IP')\n",
    "\n",
    "    for ip, group in grouped:\n",
    "        urls = group['Normalized_URL'].tolist()\n",
    "        for i in range(max_order, len(urls)):  # Pastikan cukup data untuk max_order\n",
    "            # Siapkan input dari probabilitas prediksi setiap orde\n",
    "            input_vector = []\n",
    "            for order in range(1, max_order + 1):\n",
    "                state = tuple(urls[i-order:i])  # State untuk orde tertentu\n",
    "                if state in markov_models[order]:\n",
    "                    input_vector.append(markov_models[order][state].get(urls[i], 0))  # Probabilitas URL berikutnya\n",
    "                else:\n",
    "                    input_vector.append(0)  # Jika state tidak ditemukan\n",
    "            \n",
    "            # Target adalah URL berikutnya\n",
    "            target_url = urls[i]\n",
    "            \n",
    "            # Masukkan ke dataset\n",
    "            X.append(input_vector)\n",
    "            y.append(target_url)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Neural Network selesai disiapkan:\n",
      "X_train shape: (693061, 4), y_train shape: (693061,)\n",
      "X_test shape: (170860, 4), y_test shape: (170860,)\n"
     ]
    }
   ],
   "source": [
    "# Siapkan dataset Neural Network\n",
    "X_train, y_train = prepare_nn_dataset(training_data, markov_models, max_order=max_order)\n",
    "X_test, y_test = prepare_nn_dataset(testing_data, markov_models, max_order=max_order)\n",
    "\n",
    "print(f\"Dataset Neural Network selesai disiapkan:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kelas unik (URL): 1874\n"
     ]
    }
   ],
   "source": [
    "# Encode target (y_train dan y_test)\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "print(f\"Jumlah kelas unik (URL): {len(encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melatih Neural Network...\n",
      "Iteration 1, loss = 4.27305226\n",
      "Iteration 2, loss = 3.90225977\n",
      "Iteration 3, loss = 3.82137598\n",
      "Iteration 4, loss = 3.77939442\n",
      "Iteration 5, loss = 3.74928017\n",
      "Iteration 6, loss = 3.72577778\n",
      "Iteration 7, loss = 3.70497983\n",
      "Iteration 8, loss = 3.68570729\n",
      "Iteration 9, loss = 3.66745998\n",
      "Iteration 10, loss = 3.65128779\n",
      "Iteration 11, loss = 3.63719737\n",
      "Iteration 12, loss = 3.62541096\n",
      "Iteration 13, loss = 3.61332842\n",
      "Iteration 14, loss = 3.60338641\n",
      "Iteration 15, loss = 3.59369980\n",
      "Iteration 16, loss = 3.58484773\n",
      "Iteration 17, loss = 3.57730791\n",
      "Iteration 18, loss = 3.57073077\n",
      "Iteration 19, loss = 3.56436059\n",
      "Iteration 20, loss = 3.55861620\n",
      "Iteration 21, loss = 3.55422101\n",
      "Iteration 22, loss = 3.54870075\n",
      "Iteration 23, loss = 3.54434812\n",
      "Iteration 24, loss = 3.54010050\n",
      "Iteration 25, loss = 3.53632412\n",
      "Iteration 26, loss = 3.53232876\n",
      "Iteration 27, loss = 3.52864685\n",
      "Iteration 28, loss = 3.52457915\n",
      "Iteration 29, loss = 3.52133351\n",
      "Iteration 30, loss = 3.51775015\n",
      "Iteration 31, loss = 3.51421551\n",
      "Iteration 32, loss = 3.51074236\n",
      "Iteration 33, loss = 3.50701540\n",
      "Iteration 34, loss = 3.50411962\n",
      "Iteration 35, loss = 3.50052057\n",
      "Iteration 36, loss = 3.49648675\n",
      "Iteration 37, loss = 3.49326312\n",
      "Iteration 38, loss = 3.49001240\n",
      "Iteration 39, loss = 3.48635001\n",
      "Iteration 40, loss = 3.48292871\n",
      "Iteration 41, loss = 3.48024711\n",
      "Iteration 42, loss = 3.47603821\n",
      "Iteration 43, loss = 3.47239378\n",
      "Iteration 44, loss = 3.46880863\n",
      "Iteration 45, loss = 3.46592494\n",
      "Iteration 46, loss = 3.46249190\n",
      "Iteration 47, loss = 3.45949567\n",
      "Iteration 48, loss = 3.45600923\n",
      "Iteration 49, loss = 3.45362978\n",
      "Iteration 50, loss = 3.45039468\n",
      "Iteration 51, loss = 3.44725081\n",
      "Iteration 52, loss = 3.44408027\n",
      "Iteration 53, loss = 3.44184105\n",
      "Iteration 54, loss = 3.43860603\n",
      "Iteration 55, loss = 3.43667630\n",
      "Iteration 56, loss = 3.43424589\n",
      "Iteration 57, loss = 3.43146441\n",
      "Iteration 58, loss = 3.42850771\n",
      "Iteration 59, loss = 3.42670533\n",
      "Iteration 60, loss = 3.42430489\n",
      "Iteration 61, loss = 3.42173543\n",
      "Iteration 62, loss = 3.41970159\n",
      "Iteration 63, loss = 3.41803460\n",
      "Iteration 64, loss = 3.41611945\n",
      "Iteration 65, loss = 3.41352520\n",
      "Iteration 66, loss = 3.41189984\n",
      "Iteration 67, loss = 3.40996387\n",
      "Iteration 68, loss = 3.40711382\n",
      "Iteration 69, loss = 3.40523019\n",
      "Iteration 70, loss = 3.40305305\n",
      "Iteration 71, loss = 3.40126528\n",
      "Iteration 72, loss = 3.39960962\n",
      "Iteration 73, loss = 3.39740256\n",
      "Iteration 74, loss = 3.39575358\n",
      "Iteration 75, loss = 3.39473990\n",
      "Iteration 76, loss = 3.39302149\n",
      "Iteration 77, loss = 3.39135384\n",
      "Iteration 78, loss = 3.38969407\n",
      "Iteration 79, loss = 3.38855068\n",
      "Iteration 80, loss = 3.38735219\n",
      "Iteration 81, loss = 3.38552064\n",
      "Iteration 82, loss = 3.38471510\n",
      "Iteration 83, loss = 3.38278475\n",
      "Iteration 84, loss = 3.38196361\n",
      "Iteration 85, loss = 3.38093896\n",
      "Iteration 86, loss = 3.37871880\n",
      "Iteration 87, loss = 3.37808304\n",
      "Iteration 88, loss = 3.37668446\n",
      "Iteration 89, loss = 3.37592248\n",
      "Iteration 90, loss = 3.37490863\n",
      "Iteration 91, loss = 3.37373112\n",
      "Iteration 92, loss = 3.37268114\n",
      "Iteration 93, loss = 3.37148630\n",
      "Iteration 94, loss = 3.37121549\n",
      "Iteration 95, loss = 3.37015629\n",
      "Iteration 96, loss = 3.36919350\n",
      "Iteration 97, loss = 3.36820002\n",
      "Iteration 98, loss = 3.36723588\n",
      "Iteration 99, loss = 3.36682443\n",
      "Iteration 100, loss = 3.36602666\n",
      "Pelatihan selesai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamzah/.pyenv/versions/3.11.6/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Bangun neural network dengan 2 hidden layers\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # Ukuran hidden layer\n",
    "    max_iter=100,                # Maksimal iterasi\n",
    "    random_state=42,             # Seed untuk reproduksibilitas\n",
    "    verbose=True,                # Tampilkan log pelatihan\n",
    "    learning_rate_init=0.001     # Learning rate awal\n",
    ")\n",
    "\n",
    "# Latih neural network\n",
    "print(\"Melatih Neural Network...\")\n",
    "mlp.fit(X_train, y_train_encoded)\n",
    "print(\"Pelatihan selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 33.48%\n",
      "Test Accuracy: 27.36%\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model\n",
    "train_accuracy = mlp.score(X_train, y_train_encoded)\n",
    "test_accuracy = mlp.score(X_test, y_test_encoded)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dan encoder telah disimpan.\n"
     ]
    }
   ],
   "source": [
    "# Simpan model\n",
    "with open(\"neural_network_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(mlp, model_file)\n",
    "\n",
    "# Simpan encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as encoder_file:\n",
    "    pickle.dump(encoder, encoder_file)\n",
    "\n",
    "print(\"Model dan encoder telah disimpan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi prediksi Neural Network: (170860, 1874)\n"
     ]
    }
   ],
   "source": [
    "# Mendapatkan Output Neural Network\n",
    "# Prediksi probabilitas menggunakan Neural Network\n",
    "nn_predictions = mlp.predict_proba(X_test)\n",
    "\n",
    "# Cek dimensi hasil prediksi\n",
    "print(f\"Dimensi prediksi Neural Network: {nn_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrasi Output Neural Network ke Multi-Order Markov Chains\n",
    "def hybrid_predict(current_state, markov_models, nn_prediction, max_order=4):\n",
    "    \"\"\"\n",
    "    Membuat prediksi hybrid berdasarkan Multi-Order Markov Chains dan Neural Network.\n",
    "\n",
    "    Args:\n",
    "        current_state (tuple): State saat ini untuk Markov Chains.\n",
    "        markov_models (dict): Dictionary Multi-Order Markov Chains.\n",
    "        nn_prediction (array): Prediksi probabilitas Neural Network untuk URL berikutnya.\n",
    "        max_order (int): Orde maksimum dari Markov Chains.\n",
    "\n",
    "    Returns:\n",
    "        list: URL yang diprediksi (berdasarkan probabilitas tertinggi).\n",
    "    \"\"\"\n",
    "    hybrid_scores = defaultdict(float)\n",
    "\n",
    "    # Gabungkan probabilitas dari setiap Markov Chain dengan bobot dari Neural Network\n",
    "    for order in range(1, max_order + 1):\n",
    "        state = tuple(current_state[-order:])  # State untuk orde tertentu\n",
    "        if state in markov_models[order]:\n",
    "            for next_url, prob in markov_models[order][state].items():\n",
    "                nn_index = encoder.transform([next_url])[0]  # Cari indeks URL di encoder\n",
    "                hybrid_scores[next_url] += prob * nn_prediction[nn_index]\n",
    "\n",
    "    # Urutkan berdasarkan skor hybrid\n",
    "    sorted_predictions = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [url for url, score in sorted_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Prediction Accuracy (Top-1): 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Hybrid Prediction\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    current_state = X_test[i]  # State saat ini\n",
    "    actual_next_url = y_test[i]  # URL berikutnya yang sebenarnya\n",
    "\n",
    "    # Prediksi hybrid\n",
    "    nn_prediction = nn_predictions[i]\n",
    "    predicted_urls = hybrid_predict(current_state, markov_models, nn_prediction, max_order=4)\n",
    "\n",
    "    if predicted_urls and actual_next_url in predicted_urls[:1]:  # Top-1 prediction\n",
    "        correct_predictions += 1\n",
    "    total_predictions += 1\n",
    "\n",
    "# Hitung akurasi\n",
    "hybrid_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"Hybrid Prediction Accuracy (Top-1): {hybrid_accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
